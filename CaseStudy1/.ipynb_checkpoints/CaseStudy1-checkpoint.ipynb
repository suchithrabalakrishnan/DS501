{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1 : Collecting Data from Twitter\n",
    "\n",
    "Due Date: September 22, **before the beginning of class at 6:00pm**\n",
    "\n",
    "* ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/9/9f/Twitter_bird_logo_2012.svg/220px-Twitter_bird_logo_2012.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    member 1\n",
    "    \n",
    "    member 2\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* Chapter 1 and Chapter 9 of the book [Mining the Social Web](http://www.learndatasci.com/wp-content/uploads/2015/08/Mining-the-Social-Web-2nd-Edition.pdf) \n",
    "* The codes for [Chapter 1](http://bit.ly/1qCtMrr) and [Chapter 9](http://bit.ly/1u7eP33)\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Sampling Twitter Data with Streaming API about a certain topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select a topic that you are interested in, for example, \"WPI\" or \"Lady Gaga\"\n",
    "* Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
    "* Store the tweets you downloaded into a local file (txt file or json file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of statuses 100\n",
      "Length of statuses 200\n",
      "Length of statuses 300\n",
      "Length of statuses 390\n",
      "Tweets output to file successfully!\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "import time\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "#---------------------------------------------\n",
    "# Define a Function to Login Twitter API\n",
    "def oauth_login():\n",
    "    # Go to http://twitter.com/apps/new to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "    # on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = 'iEG3V3LLXNerRncmEWgv8lQ4n'\n",
    "    CONSUMER_SECRET ='67HGDVQRF7YxJ6ZUFnUlvwreR2CShhao1Zx6k5wo0OoRRkGez4'\n",
    "    OAUTH_TOKEN = '2380166015-sZywqhm3U9naQ154u0qQQzBv1FJgAWDVSWN6ROB'\n",
    "    OAUTH_TOKEN_SECRET = '1AQCSvqp06DTpwYbgBXooWVmLuzWJ8zpsFpWgaazUhhha'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "import json\n",
    "\n",
    "# login to api\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# set initial search\n",
    "search_results = twitter_api.search.tweets(q='driverless', count=100, lang='en')\n",
    "statuses = search_results['statuses']\n",
    "print(\"Length of statuses\", len(statuses))\n",
    "max_tweets = 1000\n",
    "\n",
    "while(len(statuses) < max_tweets):\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError: # No more results when next_results doesn't exist\n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ])\n",
    "    \n",
    "    # search api will take the key words dictionary\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses += search_results['statuses']\n",
    "    print(\"Length of statuses\", len(statuses))\n",
    "\n",
    "# Write the status to a local file\n",
    "outfile = open('tweets.txt', 'w')\n",
    "outfile.write(json.dumps(statuses, indent=1))\n",
    "outfile.close()\n",
    "print (\"Tweets output to file successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report some statistics about the tweets you collected \n",
    "\n",
    "* The topic of interest: < INSERT YOUR TOPIC HERE>\n",
    "\n",
    "\n",
    "* The total number of tweets collected:  < INSERT THE NUMBER HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Analyzing Tweets and Tweet Entities with Frequency Analysis\n",
    "\n",
    "**1. Word Count:** \n",
    "* Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets. \n",
    "* Plot a table of the top 30 words with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|     Word     | Count |\n",
      "+--------------+-------+\n",
      "|      of      |   203 |\n",
      "|      to      |   196 |\n",
      "|     cars     |   188 |\n",
      "|     for      |   181 |\n",
      "|  driverless  |   171 |\n",
      "|     Uber     |   101 |\n",
      "|     the      |    97 |\n",
      "|      a       |    93 |\n",
      "|     tech     |    92 |\n",
      "|   Seattle    |    91 |\n",
      "| self-driving |    89 |\n",
      "|     Two      |    88 |\n",
      "|     vets     |    84 |\n",
      "|     off      |    82 |\n",
      "|     150      |    80 |\n",
      "|    miles     |    80 |\n",
      "|    block     |    80 |\n",
      "|  interstate  |    80 |\n",
      "|     have     |    73 |\n",
      "|    could     |    72 |\n",
      "+--------------+-------+\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# Collect status texts\n",
    "status_texts = [ status['text'] \n",
    "                 for status in statuses ]\n",
    "\n",
    "# Use word to store all words in tweets and use counter to do analysis\n",
    "word = []\n",
    "for text in status_texts:\n",
    "    for w in text.split():\n",
    "        word.append(w)\n",
    "word_freq = Counter(word)\n",
    "\n",
    "# Put the data into the prettytable\n",
    "top_word = PrettyTable(field_names=['Word','Count']) # Header\n",
    "row_num = 0 # number of rows in table\n",
    "for item in word_freq.most_common()[:20]:\n",
    "    top_word.add_row(item) # Adding rows\n",
    "top_word.align['Word'], top_word.align['Count'] = 'c', 'r' # Set column alignment\n",
    "print (top_word)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the most popular tweets in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------------------------------------------------+\n",
      "| Count | Screen Name   | Tweet Text                                         |\n",
      "+-------+---------------+----------------------------------------------------+\n",
      "| 20    | DigitalTrends | RT @DigitalTrends: Oxbotica is developing its own  |\n",
      "|       |               | driverless cars across the pond                    |\n",
      "|       |               | https://t.co/wA04kBv5TV https://t.co/2dM7BuSDmo    |\n",
      "| 14    | BrookingsInst | RT @BrookingsInst: Did the recession keep bad      |\n",
      "|       |               | drivers off the road? https://t.co/kttO25jLHQ      |\n",
      "|       |               | https://t.co/bKjyz1q71W                            |\n",
      "| 13    | DriverlessNow | RT @DriverlessNow: Self-driving cars are here, but |\n",
      "|       |               | that doesn't mean you can call them 'driverless'   |\n",
      "|       |               | https://t.co/wjjnbtsy6e @businessinsideâ€¦           |\n",
      "| 11    | JMBooyah      | RT @JMBooyah: The most common objection drivers    |\n",
      "|       |               | post in response to any of my articles on          |\n",
      "|       |               | driverless cars is \"who's going to clean the carsâ€¦ |\n",
      "| 11    | JMBooyah      | RT @JMBooyah: The most common objection drivers    |\n",
      "|       |               | post in response to any of my articles on          |\n",
      "|       |               | driverless cars is \"who's going to clean the carsâ€¦ |\n",
      "| 11    | JMBooyah      | RT @JMBooyah: The most common objection drivers    |\n",
      "|       |               | post in response to any of my articles on          |\n",
      "|       |               | driverless cars is \"who's going to clean the carsâ€¦ |\n",
      "| 11    | JMBooyah      | RT @JMBooyah: The most common objection drivers    |\n",
      "|       |               | post in response to any of my articles on          |\n",
      "|       |               | driverless cars is \"who's going to clean the carsâ€¦ |\n",
      "| 11    | JMBooyah      | RT @JMBooyah: The most common objection drivers    |\n",
      "|       |               | post in response to any of my articles on          |\n",
      "|       |               | driverless cars is \"who's going to clean the carsâ€¦ |\n",
      "| 11    | JMBooyah      | RT @JMBooyah: The most common objection drivers    |\n",
      "|       |               | post in response to any of my articles on          |\n",
      "|       |               | driverless cars is \"who's going to clean the carsâ€¦ |\n",
      "| 10    | BBCTech       | RT @BBCTech: Ride-hailing firm Lyft predicts       |\n",
      "|       |               | driverless cabs in 5 years https://t.co/URBkFdPLnN |\n",
      "+-------+---------------+----------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# Collect all retweets\n",
    "retweets = []\n",
    "for status in statuses:\n",
    "    if 'retweeted_status' in status:\n",
    "        retweets.append((status['retweet_count'], status['retweeted_status']['user']['screen_name'], status['text']))\n",
    "\n",
    "# Generate the pretty table\n",
    "most_retweet = PrettyTable(field_names=['Count', 'Screen Name', 'Tweet Text']) # Header\n",
    "for item in sorted(retweets, reverse=True)[:10]:\n",
    "    most_retweet.add_row(item) # Adding rows\n",
    "most_retweet.align['Count'], most_retweet.align['Screen Name'], most_retweet.align['Tweet Text'] = 'l', 'l', 'l'\n",
    "most_retweet.max_width['Tweet Text'] = 50 # Set max width for \"Tweet Text\" column\n",
    "print (most_retweet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the most popular Tweet Entities in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 hashtags, top 10 user mentions that are the most popular in your collection of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "| Hashtag    | Count |\n",
      "+------------+-------+\n",
      "| driverless |   21  |\n",
      "| tech       |   8   |\n",
      "| Uber       |   6   |\n",
      "| Lyft       |   5   |\n",
      "| AI         |   5   |\n",
      "| Driverless |   4   |\n",
      "| business   |   4   |\n",
      "| Seattle    |   3   |\n",
      "| Boston     |   3   |\n",
      "| Tech       |   3   |\n",
      "+------------+-------+\n",
      "+-----------------+-------+\n",
      "| Mention User    | Count |\n",
      "+-----------------+-------+\n",
      "| businessinsider |   17  |\n",
      "| Uber            |   7   |\n",
      "| JMBooyah        |   6   |\n",
      "| SAI             |   5   |\n",
      "| FerRomero_FREE  |   5   |\n",
      "| evankirstel     |   4   |\n",
      "| Laraba811       |   4   |\n",
      "| markets         |   4   |\n",
      "| DigitalTrends   |   3   |\n",
      "| techradar       |   3   |\n",
      "+-----------------+-------+\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# Collect hashtags\n",
    "hashtags = [ hashtag['text'] \n",
    "             for status in statuses\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Collect mention user\n",
    "mention_user = [ user_mention['screen_name'] \n",
    "                 for status in statuses\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "# Generate pretty table\n",
    "for label, data in (('Hashtag',hashtags), ('Mention User', mention_user)): # Access two table data\n",
    "    table = PrettyTable(field_names=[label, 'Count']) # Header\n",
    "    count = Counter(data) # Count the data\n",
    "    for item in count.most_common()[:10]:\n",
    "        table.add_row(item) # Adding each row\n",
    "    table.align[label], table.align['Count'] = 'l', 'c'\n",
    "    print (table)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ------------------------\n",
    "\n",
    "# Problem 3: Getting \"All\" friends and \"All\" followers of a popular user in twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "* Get the list of all friends and all followers of the twitter user.\n",
    "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# Get all friends\n",
    "friends = twitter_api.friends.ids(screen_name='driverless', count=5000)\n",
    "friend_list = friends['ids']\n",
    "next_cursor = friends['next_cursor']\n",
    "while (next_cursor != 0 and len(friend_list) < 5000*15): # Navigate through pages by cursor and rate controller\n",
    "    friends = twitter_api.friends.ids(screen_name='driverless', cursor=next_cursor, count=5000)\n",
    "    friend_list += friends['ids']\n",
    "    next_cursor = friends['next_cursor']\n",
    "\n",
    "\n",
    "# Get all followers\n",
    "followers = twitter_api.followers.ids(screen_name='driverless', count=5000)\n",
    "follower_list = followers['ids']\n",
    "next_cursor = followers['next_cursor']\n",
    "rate_counter = 1\n",
    "while (next_cursor != 0 and len(follower_list) < 5000*15): # Navigate through pages by cursor and rate controller\n",
    "    followers = twitter_api.followers.ids(screen_name='driverless', cursor=next_cursor, count=5000)\n",
    "    follower_list += followers['ids']\n",
    "    next_cursor = followers['next_cursor']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "| Friend ID |   Screen Name   |\n",
      "+-----------+-----------------+\n",
      "| 46574977  |    ChineseWSJ   |\n",
      "| 379896835 |    Sleep_Sha    |\n",
      "| 77830070  |     Candg84     |\n",
      "| 131331706 | estherruthneica |\n",
      "| 129237555 |      avb001     |\n",
      "| 42446023  |    baozuitun    |\n",
      "| 14581421  |    wenyunchao   |\n",
      "| 15494014  |    roseluqiu    |\n",
      "| 110128718 |   wuhongfei99   |\n",
      "| 2863771   |    turingbook   |\n",
      "| 50969839  |    laoyang945   |\n",
      "| 105816280 |    himemeizhi   |\n",
      "| 126907973 |   aphraseaweek  |\n",
      "| 18949686  |     NanZhou     |\n",
      "| 34546831  |    chang_ping   |\n",
      "| 45630895  |     chengr28    |\n",
      "| 63131101  |    sdelayang    |\n",
      "| 93550990  |      rthkhk     |\n",
      "+-----------+-----------------+\n",
      "+--------------------+-----------------+\n",
      "| Follower ID        |   Screen Name   |\n",
      "+--------------------+-----------------+\n",
      "| 4328061975         |   Brandere_com  |\n",
      "| 3306989270         |  DueDiligences  |\n",
      "| 700555031175516160 |     ewgy6u1     |\n",
      "| 87647943           |    ever_feng    |\n",
      "| 529751069          |    digi_cars    |\n",
      "| 2727774182         | best_driverless |\n",
      "| 2395936465         |     plum_283    |\n",
      "| 97364130           |    vidiwaruo    |\n",
      "| 558754581          |      zucbg      |\n",
      "| 161956016          |    sonicyard    |\n",
      "| 202525254          |   freedomlity   |\n",
      "| 313404062          |    ckong1983    |\n",
      "| 307219031          |    Stonechen2   |\n",
      "| 258594213          |   mengtianlian  |\n",
      "| 154063607          |     cnmaoist    |\n",
      "| 263087080          |  RobertEptingde |\n",
      "| 109527074          |      ccav77     |\n",
      "| 254504134          |  Pembroke_Chung |\n",
      "+--------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "# Create table for friend\n",
    "friend_table = PrettyTable(field_names=['Friend ID', 'Screen Name']) # Header\n",
    "for user in twitter_api.users.lookup(user_id=friend_list[:20]):\n",
    "    friend_table.add_row([user['id'], user['screen_name']])\n",
    "friend_table.align['Friend ID'], friend_table.align['Screen Name'] = 'l', 'c' # Alignment\n",
    "print (friend_table)\n",
    "\n",
    "# Create table for follower\n",
    "follower_table = PrettyTable(field_names=['Follower ID', 'Screen Name']) # Header\n",
    "for user in twitter_api.users.lookup(user_id=follower_list[:20]):\n",
    "    follower_table.add_row([user['id'], user['screen_name']])\n",
    "follower_table.align['Follower ID'], follower_table.align['Screen Name'] = 'l', 'c' # Alignment\n",
    "print (follower_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "| Mutual Friend ID |  Screen Name   |\n",
      "+------------------+----------------+\n",
      "| 109527074        |     ccav77     |\n",
      "| 161965669        |  angelia_hsu   |\n",
      "| 263087080        | RobertEptingde |\n",
      "| 161956016        |   sonicyard    |\n",
      "| 111259604        |    pyx1963     |\n",
      "+------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# Create Set for friend and follower\n",
    "friend = set(friend_list)\n",
    "follower = set(follower_list)\n",
    "\n",
    "# Check mutual friend\n",
    "mutual_friend = friend.intersection(follower)\n",
    "mutual_friend = list(mutual_friend)\n",
    "if (len(mutual_friend) == 0):\n",
    "    print (\"There are no mutual friends.\")\n",
    "else:\n",
    "    mutual_table = PrettyTable(field_names=['Mutual Friend ID', 'Screen Name']) # Create table for mutual friend\n",
    "    for mutual in twitter_api.users.lookup(user_id=mutual_friend[:30]):\n",
    "        mutual_table.add_row([mutual['id'], mutual['screen_name']])\n",
    "    mutual_table.align['Mutual Friend ID'], mutual_table.align['Screen Name'] = 'l', 'c'\n",
    "    print (mutual_table)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "# Problem 4: Business question \n",
    "\n",
    "Run some additional experiments with your data to gain familiarity with the twitter data and twitter API.\n",
    "\n",
    "* Come up with a business question that Twitter data could help answer.\n",
    "* Decribe the business case.\n",
    "* How could Twitter data help a company decide how to spend its resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "import twitter\n",
    "import json\n",
    "import pymongo\n",
    "#---------------------------------------------\n",
    "# Define a Function to Login Twitter API\n",
    "def oauth_login():\n",
    "    # Go to http://twitter.com/apps/new to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "    # on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = 'iEG3V3LLXNerRncmEWgv8lQ4n'\n",
    "    CONSUMER_SECRET ='67HGDVQRF7YxJ6ZUFnUlvwreR2CShhao1Zx6k5wo0OoRRkGez4'\n",
    "    OAUTH_TOKEN = '2380166015-sZywqhm3U9naQ154u0qQQzBv1FJgAWDVSWN6ROB'\n",
    "    OAUTH_TOKEN_SECRET = '1AQCSvqp06DTpwYbgBXooWVmLuzWJ8zpsFpWgaazUhhha'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "def mongoDB_connect():\n",
    "    client = pymongo.MongoClient()\n",
    "    return client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# login to api\n",
    "twitter_api = oauth_login()\n",
    "# set up database\n",
    "client = mongoDB_connect()\n",
    "db = client.ds501case1\n",
    "collection = db.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Data are cleaned\n",
      "Final volume of problem 4 tweets is 5754\n",
      "Problem 4 database is generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# main process to connect database and collect new data\n",
    "def collecte_DB(max_tweets, key_word, clean_history):\n",
    "    # parameter initial\n",
    "    index = 0\n",
    "    statuses = []\n",
    "    \n",
    "    # clean history data\n",
    "    if clean_history:\n",
    "        collection.delete_many({})\n",
    "        print 'History Data are cleaned'\n",
    "\n",
    "    for index in range(len(key_word)):\n",
    "        # initial search\n",
    "        word = key_word[index]\n",
    "        search_results = twitter_api.search.tweets(q=word, count=100, lang='en')\n",
    "        statuses += search_results['statuses']\n",
    "\n",
    "        # continue get result\n",
    "        while(len(statuses) < (max_tweets*(index+1))):\n",
    "            try:\n",
    "                next_results = search_results['search_metadata']['next_results']\n",
    "            except KeyError: # No more results when next_results doesn't exist\n",
    "                break\n",
    "\n",
    "            # Create a dictionary from next_results, which has the following form:\n",
    "            # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "            kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ])\n",
    "\n",
    "            # search api will take the key words dictionary\n",
    "            search_results = twitter_api.search.tweets(**kwargs)\n",
    "            statuses += search_results['statuses']\n",
    "    print \"Final volume of problem 4 tweets is %d\" % len(statuses)\n",
    "\n",
    "    # write data into mongoDB\n",
    "    for status in statuses:\n",
    "        res = collection.insert_one(status)\n",
    "    print \"Problem 4 database is generated successfully!\"\n",
    "\n",
    "    \n",
    "# set initial parameters\n",
    "max_tweets = 1000\n",
    "key_word = ['driverless',\n",
    "            'â€ªAutonomousCaRRâ€ª',\n",
    "            'googlecar',\n",
    "            'autonomouscars',\n",
    "            'selfdriving',\n",
    "            'futuristiccars',\n",
    "            'driverlesstechnology',\n",
    "            'SelfDrivingUber',\n",
    "            'UGV']\n",
    "\n",
    "# collect data and write into DB, enable this line iff new tweet data is necessary\n",
    "#collecte_DB(max_tweets, key_word, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sematic Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# load words pool from Bing Liu\n",
    "pos_pool = pd.read_table('positive-words.txt', encoding='ISO-8859-1')\n",
    "neg_pool = pd.read_table('negative-words.txt', encoding='ISO-8859-1')\n",
    "pos_pool.columns = ['word']\n",
    "neg_pool.columns = ['word']\n",
    "pos_pool = pos_pool['word'].tolist() \n",
    "neg_pool = neg_pool['word'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# function to extract words inside the tweet\n",
    "def extract_word(tweet):\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    words = [e.lower() for e in nltk.tokenize.word_tokenize(tweet) \n",
    "             if ((len(e) >= 3) and (e not in stop) and (not e.startswith(('#','@'))))]\n",
    "    return (words)\n",
    "\n",
    "\n",
    "\n",
    "# build word counter, co-occurance matrix\n",
    "com = defaultdict(lambda: defaultdict(int))\n",
    "words_pool = []\n",
    "stop = nltk.corpus.stopwords.words('english') + list(string.punctuation) + ['RT', 'rt', 'via']\n",
    "\n",
    "for tweet in collection.find({}):\n",
    "    text = extract_word(tweet['text']) # extract words\n",
    "    \n",
    "    words_pool += text # build words pool\n",
    "    \n",
    "    for i in range(len(text)-1): # build co-occurance matrix\n",
    "        for j in range(i+1, len(text)):\n",
    "            w1, w2 = sorted([text[i], text[j]])\n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1\n",
    "\n",
    "words_counter = Counter(words_pool)\n",
    "\n",
    "\n",
    "\n",
    "# compute word prob\n",
    "p_w = {}\n",
    "p_com = defaultdict(lambda : defaultdict(int))\n",
    "n_pool = float(len(words_pool))\n",
    " \n",
    "for word, n in words_counter.items():\n",
    "    p_w[word] = n / n_pool\n",
    "    for cow in com[word]:\n",
    "        p_com[word][cow] = com[word][cow] / n_pool\n",
    "\n",
    "        \n",
    "               \n",
    "# compute pmi and semantic orientation\n",
    "pmi = defaultdict(lambda : defaultdict(int))\n",
    "for w1 in p_w:\n",
    "    for w2 in com[w1]:\n",
    "        denom = p_w[w1] * p_w[w2]\n",
    "        pmi[w1][w2] = np.log2(p_com[w1][w2] / denom)\n",
    "\n",
    "SO_m = {}\n",
    "for word, n in p_w.items():\n",
    "    positive_assoc = sum(pmi[word][tx] for tx in pos_pool)\n",
    "    negative_assoc = sum(pmi[word][tx] for tx in neg_pool)\n",
    "    SO_m[word] = positive_assoc - negative_assoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 co-occurant word with google\n",
      "+---------------------+---------------+\n",
      "|         Word        |   PMI Value   |\n",
      "+---------------------+---------------+\n",
      "|      techshare      |  8.8468042303 |\n",
      "|         rnib        |  8.8468042303 |\n",
      "|         neat        |  8.8468042303 |\n",
      "|      googlemaps     |  8.8468042303 |\n",
      "|       surveys       |  8.8468042303 |\n",
      "| googleautonomouscar | 8.58376982446 |\n",
      "|        indeed       | 8.26184172957 |\n",
      "|        loses        | 7.98430775405 |\n",
      "|       patents       | 7.90569791935 |\n",
      "|        hires        |  7.8468042303 |\n",
      "|       respond       |  7.8468042303 |\n",
      "|    selfdrivngcar    |  7.8468042303 |\n",
      "|      googlebike     |  7.8468042303 |\n",
      "|        solved       |  7.8468042303 |\n",
      "|        â€œthey        |  7.8468042303 |\n",
      "|        hinges       |  7.8468042303 |\n",
      "|        roger        |  7.8468042303 |\n",
      "|        tight        |  7.8468042303 |\n",
      "|       mbrainfr      |  7.8468042303 |\n",
      "|       technica      |  7.8468042303 |\n",
      "+---------------------+---------------+\n",
      "The top 20 co-occurant word with tesla\n",
      "+------------------+---------------+\n",
      "|       Word       |   PMI Value   |\n",
      "+------------------+---------------+\n",
      "|    upgrading     | 8.81564329881 |\n",
      "|     unwanted     | 7.81564329881 |\n",
      "|     whereskr     | 7.81564329881 |\n",
      "|      wbrjln      | 7.81564329881 |\n",
      "|      warned      | 7.81564329881 |\n",
      "|  updatedsystem   | 7.81564329881 |\n",
      "|     â€˜pushing     | 7.81564329881 |\n",
      "|      victim      | 7.81564329881 |\n",
      "|     together     | 7.81564329881 |\n",
      "|    topiclyav     | 7.81564329881 |\n",
      "|      â€˜tesla      | 7.81564329881 |\n",
      "|  todaytonightsa  | 7.81564329881 |\n",
      "|   whitelisted    | 7.81564329881 |\n",
      "|      twist       | 7.81564329881 |\n",
      "| theteslachannel  | 7.81564329881 |\n",
      "|       www        | 7.81564329881 |\n",
      "| teslasoftware8.0 | 7.81564329881 |\n",
      "|       v8.0       | 7.81564329881 |\n",
      "|     upgraded     | 7.81564329881 |\n",
      "|     upgrades     | 7.81564329881 |\n",
      "+------------------+---------------+\n",
      "The top 20 co-occurant word with uber\n",
      "+----------------+---------------+\n",
      "|      Word      |   PMI Value   |\n",
      "+----------------+---------------+\n",
      "|      ðŸ‘ðŸ‘ðŸ‘       | 5.36746520869 |\n",
      "|  vonniequinn   | 5.36746520869 |\n",
      "|  uberchopper   | 5.36746520869 |\n",
      "|    uber_rsa    | 5.36746520869 |\n",
      "|    uberatc     | 5.36746520869 |\n",
      "|    workflow    | 5.36746520869 |\n",
      "|     â€˜robo      | 5.36746520869 |\n",
      "| webuyurrideinc | 5.36746520869 |\n",
      "|    watching    | 5.21546211525 |\n",
      "|      yep       |  5.0455371138 |\n",
      "|      zero      | 4.95242770941 |\n",
      "|    ushering    | 4.95242770941 |\n",
      "|    zoltapp     | 4.54803745433 |\n",
      "|      wjxt      | 4.36746520869 |\n",
      "|   uber-cool    | 4.36746520869 |\n",
      "|    welcomes    | 4.36746520869 |\n",
      "|     wells      | 4.36746520869 |\n",
      "|    victoria    | 4.36746520869 |\n",
      "|   workforce    | 4.36746520869 |\n",
      "|      wral      | 4.36746520869 |\n",
      "+----------------+---------------+\n",
      "The top 20 co-occurant word with apple\n",
      "+--------------+---------------+\n",
      "|     Word     |   PMI Value   |\n",
      "+--------------+---------------+\n",
      "|   reported   | 9.05665139831 |\n",
      "|    bumps     | 9.05665139831 |\n",
      "|   canning    | 9.05665139831 |\n",
      "|   involves   | 9.05665139831 |\n",
      "|     itsâ€¦     | 9.05665139831 |\n",
      "| titanproject | 9.05665139831 |\n",
      "|     onâ€¦      | 9.05665139831 |\n",
      "|  mysterious  | 9.05665139831 |\n",
      "|     itâ€¦      | 9.05665139831 |\n",
      "| bobmcmillan  | 9.05665139831 |\n",
      "|   tribune    | 9.05665139831 |\n",
      "|   express    | 8.64161389903 |\n",
      "|   tapping    | 8.54207822548 |\n",
      "|   â€˜rebootâ€™   |  8.3785794932 |\n",
      "|   pakistan   |  8.3785794932 |\n",
      "|  rethinking  | 8.26815550351 |\n",
      "|     lays     | 8.15974489128 |\n",
      "|   strategy   | 8.13193952562 |\n",
      "|   elements   | 8.05665139831 |\n",
      "|   workers    | 8.05665139831 |\n",
      "+--------------+---------------+\n",
      "The top 20 co-occurant word with ford\n",
      "+---------------+---------------+\n",
      "|      Word     |   PMI Value   |\n",
      "+---------------+---------------+\n",
      "|      unit     | 8.53118990934 |\n",
      "|   yahoonews   | 8.53118990934 |\n",
      "|     pumps     | 8.53118990934 |\n",
      "|    fusions    | 8.11615241006 |\n",
      "|   markfields  | 8.01661673651 |\n",
      "|      oval     | 7.94622740862 |\n",
      "|      path     | 7.94622740862 |\n",
      "|    velodyne   | 7.94622740862 |\n",
      "|   on-demand   | 7.79422431517 |\n",
      "|   ride-hail   | 7.53118990934 |\n",
      "|   rideshare   | 7.53118990934 |\n",
      "| keithnaughton | 7.53118990934 |\n",
      "|    intends    | 7.53118990934 |\n",
      "|  topgear_bbca | 7.53118990934 |\n",
      "| gregbensinger | 7.53118990934 |\n",
      "|    societal   | 7.53118990934 |\n",
      "|   showrooms   | 7.53118990934 |\n",
      "|   pointcloud  | 7.53118990934 |\n",
      "|   phonebots   | 7.53118990934 |\n",
      "|    self-dr    | 7.53118990934 |\n",
      "+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "comp_name = ['google',\n",
    "             'tesla',\n",
    "             'uber',\n",
    "             'apple',\n",
    "             'ford'\n",
    "             ]\n",
    "\n",
    "for name in comp_name:\n",
    "    pmi_val = pmi[name]\n",
    "    pmi_val = sorted(pmi_val.items(),\n",
    "                     key=operator.itemgetter(1),\n",
    "                     reverse=True)\n",
    "    title = 'The top 20 co-occurant word with ' + name\n",
    "    print (title)\n",
    "    comp_table = PrettyTable(field_names=['Word', 'PMI Value'])\n",
    "    for item in pmi_val[:20]:\n",
    "        comp_table.add_row([item[0],item[1]])\n",
    "    print (comp_table)\n",
    "    \n",
    "    data_frame = pd.DataFrame(pmi_val)\n",
    "    data_frame.columns = ['Word', 'PMI Value']\n",
    "    file_name = name + '.csv'\n",
    "    data_frame.to_csv(path_or_buf=file_name, header=True, index=False, encoding='UTF-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------------+\n",
      "|    Word    | Semantic Orientation Value |\n",
      "+------------+----------------------------+\n",
      "| autonomous |       102.595997287        |\n",
      "|    day     |       74.5923936053        |\n",
      "|    auto    |       67.1979537487        |\n",
      "|    car     |       63.0891322622        |\n",
      "|    amp     |       61.3199661459        |\n",
      "|    ford    |       58.5628261828        |\n",
      "|   a-ugv    |       58.1485382108        |\n",
      "|   cross    |       53.6277545396        |\n",
      "|   autos    |       52.4839898106        |\n",
      "|   going    |       51.8391172464        |\n",
      "+------------+----------------------------+\n",
      "+------------+----------------------------+\n",
      "|    Word    | Semantic Orientation Value |\n",
      "+------------+----------------------------+\n",
      "|   apple    |       -56.1334657501       |\n",
      "|   could    |       -55.9001934622       |\n",
      "| insurance  |       -52.5995170403       |\n",
      "|    bad     |       -46.2039325324       |\n",
      "|  freedom   |       -46.0580240552       |\n",
      "|    also    |       -45.6672164765       |\n",
      "|    piss    |       -42.0580240552       |\n",
      "|    idea    |       -41.6016465549       |\n",
      "| chrisl2185 |       -40.303136553        |\n",
      "|    cops    |       -39.357584337        |\n",
      "+------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# explore the top positive and negative words\n",
    "semantic_sorted = sorted(SO_m.items(), \n",
    "                         key=operator.itemgetter(1), \n",
    "                         reverse=True)\n",
    "top_pos = semantic_sorted[:10]\n",
    "top_neg = semantic_sorted[-10:]\n",
    "top_neg.reverse()\n",
    "\n",
    "\n",
    "pos_table = PrettyTable(field_names=['Word','Semantic Orientation Value'])\n",
    "neg_table = PrettyTable(field_names=['Word','Semantic Orientation Value'])\n",
    "\n",
    "for pos,neg in zip(top_pos,top_neg):\n",
    "    pos_table.add_row([pos[0],pos[1]])\n",
    "    neg_table.add_row([neg[0],neg[1]])\n",
    "\n",
    "\n",
    "print(pos_table)\n",
    "print(neg_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "semantic_orientation = pd.DataFrame(semantic_sorted)\n",
    "semantic_orientation.columns = ['Word','Semantic Orientation Value']\n",
    "semantic_orientation.head()\n",
    "semantic_orientation.to_csv('Semantic_Orientation.csv', header=True, index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylor/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:15: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "# read in training data for NLP, this training data is downloaded from kaggle.com for tweet sentiment analysis\n",
    "nlp_train = pd.read_table('training.txt')\n",
    "\n",
    "# split train words\n",
    "train_data = []\n",
    "\n",
    "\n",
    "def extract_train_word(x, train_data):\n",
    "    words = [e.lower() for e in nltk.tokenize.word_tokenize(x['Tweet']) \n",
    "             if ((len(e) >= 3) and (e not in stop) and (not e.startswith(('#','@'))))]\n",
    "    sentiment = x['Sentiment']\n",
    "    train_data.append((words, sentiment))\n",
    "    \n",
    "result = nlp_train.apply(lambda x: extract_train_word(x, train_data), axis=1)\n",
    "\n",
    "# load words pool from Bing Liu\n",
    "pos_pool = pd.read_table('positive-words.txt')\n",
    "neg_pool = pd.read_table('negative-words.txt')\n",
    "pos_pool.columns = ['word']\n",
    "neg_pool.columns = ['word']\n",
    "word_pool = pos_pool['word'].tolist() + neg_pool['word'].tolist()\n",
    "\n",
    "# feature extraction with words pool\n",
    "def feature_extract(tweets):\n",
    "    global word_pool\n",
    "    features = {}\n",
    "    dry_tweet = set(tweets)\n",
    "    for word in word_pool:\n",
    "        features['Contain (%s)'%word] = (word in dry_tweet)\n",
    "    return features\n",
    "\n",
    "# generate training set\n",
    "train_set = nltk.classify.apply_features(feature_extract, train_data)\n",
    "\n",
    "# train classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet 'driveless car is unsafe' is negative\n"
     ]
    }
   ],
   "source": [
    "# function to extract words inside the tweet\n",
    "def extract_word(tweet):\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    words = [e.lower() for e in nltk.tokenize.word_tokenize(tweet) \n",
    "             if ((len(e) >= 3) and (e not in stop) and (not e.startswith(('#','@'))))]\n",
    "    return words\n",
    "\n",
    "\n",
    "# test classify\n",
    "result_board = ['negative', 'positive']\n",
    "\n",
    "test_tweet = 'driveless car is unsafe'\n",
    "res = classifier.classify(feature_extract(extract_word(test_tweet)))\n",
    "print 'The tweet \\'%s\\' is %s' % (test_tweet, result_board[res])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @swaaanson: Musk's greatest gift to humanity will not be driverless cars or missions to Mars. It will be inspiring a whole generation toâ€¦\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "tweet = collection.find_and_modify({'retweet_count':{'$gte':500}})\n",
    "text = extract_word(tweet['text'])\n",
    "label = result_board[classifier.classify(feature_extract(text))]\n",
    "print tweet['text']\n",
    "print label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What data you collected? \n",
    "    * Why this topic is interesting or important to you? (Motivations)\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    " \n",
    "     (please include figures or tables in the report, but no source code)\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu) *and* the TA Wen Liu (wliu3@wpi.edu).\n",
    "        \n",
    "** Note: Each team just needs to submits one submission **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Criteria:\n",
    "\n",
    "** Totoal Points: 120 **\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Notebook:  **\n",
    "    Points: 80\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Qestion 1:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) Select a topic that you are interested in.\n",
    "    Points: 6 \n",
    "    \n",
    "    (2) Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million. Please check whether the total number of tweets collected is larger than 200?\n",
    "    Points: 10 \n",
    "    \n",
    "    \n",
    "    (3) Store the tweets you downloaded into a local file (txt file or json file)\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 2:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    1. Word Count\n",
    "\n",
    "    (1) Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets.\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Plot a table of the top 30 words with their counts \n",
    "    Points: 4 \n",
    "    \n",
    "    2. Find the most popular tweets in your collection of tweets\n",
    "    plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n",
    "    Points: 4 \n",
    "    \n",
    "    3. Find the most popular Tweet Entities in your collection of tweets\n",
    "\n",
    "    (1) plot a table of the top 10 hashtags, \n",
    "    Points: 4 \n",
    "\n",
    "    (2) top 10 user mentions that are the most popular in your collection of tweets.\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 3:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Get the list of all friends and all followers of the twitter user.\n",
    "    Points: 4 \n",
    "\n",
    "    (3) Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "\n",
    "    (4) Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "    \n",
    "    (5) Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table\n",
    "    Points: 4 \n",
    "  \n",
    "    -----------------------------------\n",
    "    Qestion 4:  Business question\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "        Novelty: 10\n",
    "        Interestingness: 10\n",
    "    -----------------------------------\n",
    "    Run some additional experiments with your data to gain familiarity with the twitter data ant twitter API.  Come up with a business question and describe how Twitter data can help you answer that question.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Report: communicate the results**\n",
    "    Points: 20\n",
    "\n",
    "(1) What data you collected?\n",
    "    Points: 5 \n",
    "\n",
    "(2) Why this topic is interesting or important to you? (Motivations)\n",
    "    Points: 5 \n",
    "\n",
    "(3) How did you analyse the data?\n",
    "    Points: 5 \n",
    "\n",
    "(4) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)\n",
    "    Points: 5 \n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Slides (for 10 minutes of presentation): Story-telling **\n",
    "    Points: 20\n",
    "\n",
    "\n",
    "1. Motivation about the data collection, why the topic is interesting to you.\n",
    "    Points: 5 \n",
    "\n",
    "2. Communicating Results (figure/table)\n",
    "    Points: 10 \n",
    "\n",
    "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
    "    Points: 5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
